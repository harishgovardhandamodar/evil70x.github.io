We need to specify what data synthesis is going to protect against. We have argued previously that attribute and inferential disclosure are both forms of statistical analysis, and therefore we would not want synthesis to protect against these. We would want models to be built from synthetic data. We would want inferences to be derived from synthetic data. Both of these would pertain to groups with specific characteristics without identifying the records that belong to individuals in that dataset. The reason it is necessary to make this clear is that there have been some regulatory leanings toward requiring methods that render personal information to be non personal to also protect against attribute disclosure and inferences1.

We would want synthesis to protect against identity disclosure. This is a necessary but insufficient condition for a disclosure that would be problematic. The second condition is that there is some information gain. If both of these conditions are met, then we call this a meaningful identity disclosure.

Learning something new about a group of individuals without identifying any records can potentially be harmful to members of the group. For example, if the adversary learns that members of the group have a stigmatised disease or condition, then this can potentially be harmful. Or perhaps the model that is built from the data can be used in ways that are harmful to members of the group—for example, by discriminating against them when deciding who to give bank loans or insurance to. These are legitimate concerns, but data synthesis will not protect against them. Synthetic data that retains high utility will allow models to be built that retain the original relationships in the data. Therefore, if models from real data can be used in inappropriate ways, so can models from synthetic data. These types of concerns need to be dealt with through ethics reviews on the data and model uses. They are not going to be dealt with through changes to the synthesis process.

Whether particular information is harmful or whether the uses of models from the data are potentially discriminatory may be relative to current cultural norms and the expectations of the public, and these change over time. For example, the question of whether it is appropriate to build biobanks holding people’s DNA and use that for research and other secondary purposes was controversial a decade ago but is less so now. Therefore, these assessments are subjective, and a group of individuals who are tasked with making such ethical calls is a known way to manage these kinds of risks



----------------------------------
For example, see the discussion and references in Khaled El Emam and Cecilia Alvarez, “A Critical Appraisal of the Article 29 Working Party Opinion 05/2014 on Data Anonymization Techniques,” International Data Privacy Law 5, no. 1 (2015): 73–87.