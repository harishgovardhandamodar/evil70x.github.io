In many datasets the real data sample (the “real sample”) is a subsample from some population. This real sample may have records that match the synthetic dataset (the “synthetic sample”). This matching happens on quasi-identifiers, which are the subset of variables that an adversary can know about real people in the population. If an adversary is able to match a synthetic record with a person in the population, then this is an identity disclosure. The concept is illustrated in Figure.

![[Pasted image 20230515163820.png]]


One simple way to screen the synthesised data for records that can potentially be identifying is to identify and remove unique records in the real sample that can be matched with a unique record in the synthetic sample, with the matching done on the quasi-identifiers. For example, if there was only one 50-year-old male (the age and gender variables are the quasi-identifiers) in the real sample and only one 50-year-old male in the synthetic sample, then these two records match with certainty. The next question is whether that 50-year-old male in the real sample can be matched with someone in the population with a high probability.

The risk of identification is a function of individuals in the population, and because most real datasets represent samples from that population, this means that records that are unique in the real data are not necessarily unique in the population. For example, if there are ten 50-year-old males in the population, then there is a 1:10 chance that the real record can be correctly matched to the right individual. A very conservative approach would be to assume that if the record is unique in the real sample, then it will match correctly and with certainty with a person in the population. And if that record in turn matches a unique record in the synthetic sample, then that establishes a one-to-one mapping between the synthetic sample and an individual.

In our example from the previous chapter with using decision trees for synthesis on the hospital discharge data, we found that 4% of the records were unique in the synthetic data and were also unique in the real dataset. Therefore, these records can be removed from the synthetic dataset as a privacy protection measure.

This approach is really quite conservative and can be considered a simple first step to empirically evaluating the identity disclosure risks in a synthetic dataset. More sophisticated methods can be applied to statistically estimate the probability of matching a synthetic record to a real person, accounting for different attack methods that an adversary can use.